# Deep Multimodal Movie Trailer
Semester project in EURECOM
The aims of this research project is to investigate the automatic creation of a movie trailer using state of the art machine learning/AI techniques.  
In this project a deep convolutional neural network is trained on the task of selecting which scenes from a movie should be used for making the trailer. For this a number of trailers/movie pairs will be analysed, features extracted, and then a deep model will be trained and evaluated.  
This project follows the footsteps of the work in [1] about learning to auto encoding a movie and using the model for re-generating movies on one side, and the recent work from IBM Watson [2] and [3] which used content based audio-visual analysis and Machine Learning/AI for making the first ever computer assisted movie trailer for a Hollywood studio (20th Century Fox).
 
Working Framework: OpenCV, Tensorflow (SoundNet and ResNet), Python.
 
References:
1. https://medium.com/@Terrybroad/autoencoding-blade-runner-88941213abbe#.zh0s5f97e
2. http://www.ibm.com/blogs/think/2016/08/31/cognitive-movie-trailer/
3. https://www.youtube.com/watch?v=gJEzuYynaiw (or search for Morgan IBM Trailer on YouTube)
